# Story 3.3: Multi-LLM Expansion (ChatGPT, Gemini, Claude)

## Status
Draft

## Story
**As a** content strategist,
**I want** to track Development Seed's visibility across multiple LLMs beyond Perplexity,
**so that** I can compare our prominence across different AI platforms and prioritize optimization efforts.

## Acceptance Criteria
1. LLM sources follow the `{ name, referrer, enabled(), query() }` interface established in Story 3.2, returning the same normalized result shape
2. ChatGPT integration via OpenAI Responses API with `web_search` tool for grounded, citation-bearing results
3. Gemini integration via Google AI API with `google_search` grounding tool
4. Claude integration via Anthropic Messages API (no web search — training data only); events tagged with `data_source: 'training'` custom property to differentiate from web-grounded results
5. Each LLM source sets its own `referrer` URL (e.g., `https://chatgpt.com`, `https://gemini.google.com`, `https://claude.ai`)
6. Each source is independently enabled/disabled via environment variable (API key presence)
7. All sources reuse `src/analysis.js` from Story 3.2 via the normalized result interface
8. Plausible Sources dashboard shows side-by-side comparison of visibility across LLMs
9. Unit tests for each source's response normalization (mocked API responses)

## Implementation Order
**Recommended:** Implement in this order based on value and complexity:
1. **Gemini** — web-grounded with structured citations (most similar to Perplexity, high value)
2. **ChatGPT** — web-grounded via Responses API (different API shape, high value)
3. **Claude** — training-data only (lowest priority, different data category)

Each can be shipped independently. Claude integration is optional for v1 — defer if the team decides training-data signals aren't useful alongside web-grounded data.

## Tasks / Subtasks
- [ ] Create `src/sources/chatgpt.js` (AC: 2, 5, 6)
  - [ ] Use OpenAI **Responses API** (not Chat Completions) with `web_search` tool
  - [ ] Endpoint: `POST https://api.openai.com/v1/responses`
  - [ ] Model: `gpt-4o` or latest
  - [ ] Parse `output` array for `message` and `web_search_call` items
  - [ ] Extract cited URLs from `annotations` in the message content
  - [ ] Normalize to `{ content, citations, searchResults, usage }`
  - [ ] Set referrer to `https://chatgpt.com`
  - [ ] Enable via `OPENAI_API_KEY`
- [ ] Create `src/sources/gemini.js` (AC: 3, 5, 6)
  - [ ] Use Google Generative AI API with `google_search` tool
  - [ ] Endpoint: `POST https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent`
  - [ ] Model: `gemini-2.0-flash` or latest with grounding support
  - [ ] Enable grounding via `tools: [{ google_search: {} }]` in request body
  - [ ] Parse `groundingMetadata` from response for `groundingChunks` (contains `web` objects with `uri` and `title`)
  - [ ] Parse `groundingSupports` for segment-level source attribution
  - [ ] Normalize to `{ content, citations, searchResults, usage }`
  - [ ] Set referrer to `https://gemini.google.com`
  - [ ] Enable via `GOOGLE_AI_API_KEY`
- [ ] Create `src/sources/claude.js` (AC: 4, 5, 6)
  - [ ] Use Anthropic Messages API
  - [ ] Endpoint: `POST https://api.anthropic.com/v1/messages`
  - [ ] Model: `claude-sonnet-4-5-20250929` or latest
  - [ ] Headers: `x-api-key`, `anthropic-version: 2023-06-01`
  - [ ] Parse `content[0].text` from response
  - [ ] Normalize to `{ content, citations: [], searchResults: [], usage }` (citations always empty — no web search)
  - [ ] Set referrer to `https://claude.ai`
  - [ ] Enable via `ANTHROPIC_API_KEY`
  - [ ] **Add `data_source: 'training'` to event props** to differentiate from web-grounded sources
- [ ] Update `src/sources/index.js` barrel file (AC: 1)
  - [ ] Import all LLM sources
  - [ ] Export `getEnabledSources()` function that calls each source's `enabled()` method
- [ ] Update `.env.example` with all API key variables (AC: 6)
- [ ] Update GitHub Actions workflow to pass new secrets (AC: 6) — see Story 3.5
- [ ] Write unit tests per source (AC: 9)
  - [ ] Mock each API's response format → verify normalized output shape matches interface
  - [ ] Test `enabled()` behavior with and without env var
  - [ ] Test graceful handling of API errors (rate limit, auth failure, timeout)

## Dev Notes

### Source Interface (from Story 3.2)
All sources follow this interface. `enabled` is a **function**, not a static boolean. `rateLimitMs` and `dataSource` are used by the orchestrator (Story 3.4):
```javascript
export default {
  name: 'ChatGPT',
  referrer: 'https://chatgpt.com',
  rateLimitMs: 1000,    // minimum delay between consecutive calls to this source
  dataSource: 'web',    // 'web' for grounded sources, 'training' for Claude
  enabled: () => !!process.env.OPENAI_API_KEY,

  async query(searchTerm) {
    // Call LLM API, return normalized result
    return { content, citations, searchResults, usage };
  }
};
```

### Normalized Result Interface
Every source's `query()` must return this shape:
```javascript
{
  content: string,           // The LLM's response text
  citations: string[],       // Cited URLs (empty array if LLM doesn't provide them)
  searchResults: object[],   // Array of { title, url, snippet } (empty if not available)
  usage: { promptTokens: number, completionTokens: number, totalTokens: number }
}
```
This is the input to `analyzeResponse()` in `src/analysis.js`. Each source adapter is responsible for mapping its API's specific response structure into this shape.

### API-Specific Implementation Details

**ChatGPT (OpenAI Responses API with web_search):**
The standard Chat Completions API does NOT include web search. You must use the newer Responses API with the `web_search` tool to get grounded results:
```javascript
const response = await fetch('https://api.openai.com/v1/responses', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'gpt-4o',
    tools: [{ type: 'web_search' }],
    input: 'What are the best satellite imagery processing tools?',
  }),
});
// Response output contains web_search_call items and message items
// Citations appear as annotations within message content
```

**Gemini (Google AI with google_search grounding):**
Gemini's grounding response structure differs significantly from Perplexity:
```javascript
const response = await fetch(
  `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`,
  {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      contents: [{ parts: [{ text: 'What are the best satellite imagery processing tools?' }] }],
      tools: [{ google_search: {} }],
    }),
  }
);
// Response includes:
// candidates[0].content.parts[0].text — the response text
// candidates[0].groundingMetadata.groundingChunks — array of { web: { uri, title } }
// candidates[0].groundingMetadata.groundingSupports — segment-level attribution
```

**Claude (Anthropic Messages API):**
```javascript
const response = await fetch('https://api.anthropic.com/v1/messages', {
  method: 'POST',
  headers: {
    'x-api-key': process.env.ANTHROPIC_API_KEY,
    'anthropic-version': '2023-06-01',
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'claude-sonnet-4-5-20250929',
    max_tokens: 1024,
    messages: [{ role: 'user', content: 'What are the best satellite imagery processing tools?' }],
  }),
});
// Response: content[0].text — the response text
// No citations or grounding — text analysis only
```

### Differentiating Web-Grounded vs Training Data
Claude (and ChatGPT without web search) measures training data presence, not real-time web visibility. This is a fundamentally different signal. To avoid confusing the content team when they look at the Plausible dashboard:

- All events include a `data_source` prop: `'web'` for Perplexity/Gemini/ChatGPT-with-search, `'training'` for Claude
- This allows filtering in Plausible custom properties to separate the two categories
- Dashboard interpretation guide: web-grounded scores are actionable (improve content → score goes up). Training-data scores are lagging indicators (will only change when the LLM is retrained)

### LLM Comparison in Plausible
With each LLM sending events with its own `referrer`, the Plausible Sources dashboard shows:
```
Sources                Count
perplexity.ai           7
chatgpt.com             7
gemini.google.com       7
claude.ai               7
```
Filter by `data_source` prop to separate web-grounded from training-data results.

### Important Differences Between LLMs

| LLM | Web Search | Citations | What It Measures | `data_source` |
|-----|-----------|-----------|------------------|---------------|
| Perplexity | Yes (real-time) | Yes (URLs) | Current web visibility | `web` |
| ChatGPT | Yes (Responses API) | Yes (annotations) | Current web visibility | `web` |
| Gemini | Yes (grounding) | Yes (groundingChunks) | Current web visibility | `web` |
| Claude | No | No | Training data presence | `training` |

### Cost Estimates (daily, 7 queries each)
| LLM | Model | Estimated Daily Cost |
|-----|-------|---------------------|
| Perplexity | sonar | ~$0.002 |
| ChatGPT | gpt-4o | ~$0.02 |
| Claude | claude-sonnet | ~$0.01 |
| Gemini | gemini-2.0-flash | ~$0.005 |
| **Total** | | **~$0.04/day (~$1.20/month)** |

### Environment Variables
- `OPENAI_API_KEY` - Enables ChatGPT source
- `ANTHROPIC_API_KEY` - Enables Claude source
- `GOOGLE_AI_API_KEY` - Enables Gemini source
- (existing) `PERPLEXITY_API_KEY` - Enables Perplexity source

### Testing
- Per-source unit tests with mocked API responses:
  - ChatGPT: mock Responses API output → verify annotations extracted into `citations`
  - Gemini: mock `groundingMetadata` → verify `groundingChunks` extracted into `citations`
  - Claude: mock Messages API → verify empty `citations` array and correct `content` extraction
- Enable one LLM at a time → verify events appear with correct source in Plausible
- Enable all LLMs → verify Sources dashboard shows all four
- Disable all LLMs → verify graceful skip, no errors
- Compare prominence scores across LLMs for same query → should vary meaningfully
- Verify `data_source` prop correctly set to `web` or `training`
- Verify cost logging is accurate per LLM

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2026-02-06 | 0.1 | Story created for multi-LLM GEO visibility expansion | PO Agent |
| 2026-02-08 | 0.2 | Added API-specific implementation details (Responses API for ChatGPT, grounding for Gemini), data_source differentiation, normalized interface, implementation ordering, deferred shared analysis extraction to 3.2 | Review |
