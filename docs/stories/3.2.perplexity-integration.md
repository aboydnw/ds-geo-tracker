# Story 3.2: Perplexity Sonar Integration

## Status
Ready for Review

## Story
**As a** content strategist,
**I want** the tracker to query Perplexity Sonar with our GEO search terms and analyze the responses,
**so that** I can measure how prominently Development Seed appears in AI-generated answers.

## Acceptance Criteria
1. `src/sources/perplexity.js` queries the Perplexity Sonar API with one representative search term per configured query
2. For each query, the response text is analyzed for mentions of Development Seed and its products
3. Citations and search results from the API response are parsed for `developmentseed.org` URLs
4. A prominence score (0-100) is calculated based on: mention presence, position, recommendation language, and citation count
5. Each result is sent to Plausible via the enriched event model from Story 3.1 (`referrer`, `url`, props)
6. `PERPLEXITY_API_KEY` environment variable is required; source is skipped gracefully if not configured
7. Rate limiting: minimum 1 second between API calls to Perplexity
8. API cost is logged per run (Sonar: $1/1M input tokens, $1/1M output tokens)
9. Response analysis logic lives in a shared `src/analysis.js` module (reused by Story 3.3)
10. Prominence scoring unit tests exist and pass before this story ships

## Tasks / Subtasks
- [x] Create `src/sources/perplexity.js` module (AC: 1, 6)
  - [x] Export source following `{ name, referrer, enabled(), query() }` interface
  - [x] `enabled` is a function: `() => !!process.env.PERPLEXITY_API_KEY` (evaluates at call time, not import time — important for testability)
  - [x] Check `PERPLEXITY_API_KEY` env var for enablement
- [x] Implement Perplexity Sonar API client (AC: 1)
  - [x] POST to `https://api.perplexity.ai/chat/completions`
  - [x] Use `sonar` model
  - [x] Send the first search term from each query as the user message (7 API calls/day, not 28)
  - [x] Parse response: `choices[0].message.content`, `citations`, `search_results`
  - [x] Return a normalized result object: `{ content, citations: string[], searchResults: object[], usage }`
- [x] Create shared `src/analysis.js` module (AC: 9)
  - [x] Export `analyzeResponse(normalizedResult)` — accepts the normalized result from any LLM source
  - [x] Returns `{ mentioned, recommended, position, citationCount, prominenceScore, dsPages }`
  - [x] Scan response content for DS-related keywords (case-insensitive)
  - [x] Extract `developmentseed.org` URLs from citations and search results
  - [x] Record which specific DS pages were referenced
- [x] Implement prominence scoring in `src/analysis.js` (AC: 4)
  - [x] `mentioned` (boolean): DS or its products appear in the response text
  - [x] `position` (integer): Where in the response DS first appears (1 = first mention, etc.)
  - [x] `recommended` (boolean): Positive recommendation language near a DS mention (e.g., "recommended", "best", "popular")
  - [x] `citation_count` (integer): Number of DS URLs in citations
  - [x] `prominence_score` (0-100): Weighted composite of above factors
- [x] Send enriched events to Plausible (AC: 5)
  - [x] Set `referrer` to `https://perplexity.ai`
  - [x] Set `url` per Story 3.1 URL strategy (path-encoded under tracker domain)
  - [x] Include prominence data in props, plus `original_url` for the actual cited page
- [x] Handle configuration and errors (AC: 6, 7, 8)
  - [x] Skip source if `PERPLEXITY_API_KEY` not set (log info, don't fail)
  - [x] 1-second delay between Perplexity API calls
  - [x] Track and log token usage and estimated cost per run
  - [x] Catch and log API errors per query without blocking others
- [x] Write unit tests (AC: 10)
  - [x] `src/analysis.test.js`: test scoring with known response strings (mentioned, not mentioned, recommended, negative mention, multiple citations, zero citations)
  - [x] `src/sources/perplexity.test.js`: test response normalization with mocked API responses, test `enabled()` with and without env var

## Dev Notes

### Query Strategy: One API Call Per Query, Not Per Search Term
Each query (e.g., "titiler") has multiple search terms (e.g., "titiler", "titiler python", "titiler API", "titiler COG"). We send **one** API call per query using the first/primary search term. This keeps daily cost at 7 calls, not 28. The other search terms remain as metadata for context.

If we later want per-term granularity, that's a separate decision — it would 4x the API calls and Plausible events.

### Source Interface
All LLM sources follow a consistent interface. Note `enabled` is a **function**, not a static property. `rateLimitMs` declares the minimum delay between consecutive API calls (used by the orchestrator in Story 3.4):
```javascript
export default {
  name: 'Perplexity',
  referrer: 'https://perplexity.ai',
  rateLimitMs: 1000,  // 1s between calls to Perplexity
  dataSource: 'web',  // web-grounded source (vs 'training' for Claude)
  enabled: () => !!process.env.PERPLEXITY_API_KEY,

  async query(searchTerm) {
    // Call LLM API, return normalized result
    return { content, citations, searchResults, usage };
  }
};
```

### Normalized Result Object
Each source's `query()` method returns the same shape, regardless of how the underlying API structures its response. Each source is responsible for mapping its API's snake_case fields (e.g., Perplexity's `search_results`, `prompt_tokens`) into this camelCase interface:
```javascript
{
  content: string,           // The LLM's response text
  citations: string[],       // Array of cited URLs (empty if LLM doesn't provide them)
  searchResults: object[],   // Array of { title, url, snippet } (empty if not available)
  usage: { promptTokens: number, completionTokens: number, totalTokens: number }
}
```

### Analysis Output Object
`analyzeResponse(normalizedResult)` returns this shape. Internal property names use camelCase. When sending to Plausible, props use snake_case (Plausible convention):
```javascript
// analyzeResponse() returns:
{
  mentioned: boolean,          // DS or products found in response text
  recommended: boolean,        // Positive recommendation language detected
  position: number,            // Position of first DS mention (0 = not found)
  citationCount: number,       // Number of developmentseed.org URLs in citations
  prominenceScore: number,     // 0-100 composite score
  dsPages: string[],           // Actual DS URLs found (e.g., ['https://developmentseed.org/blog/titiler-v2'])
}

// Mapped to Plausible props as:
{
  query_name: 'satellite imagery tools',
  prominence_score: '72',       // string — Plausible props are always strings
  mentioned: 'true',
  recommended: 'true',
  position: '2',
  citation_count: '3',
  data_source: 'web',           // from source.dataSource
  original_url: 'https://developmentseed.org/blog/titiler-v2',  // first dsPages entry
}
```

**Perplexity Sonar API:**
- Endpoint: `https://api.perplexity.ai/chat/completions`
- Model: `sonar` (web-grounded, includes citations)
- Auth: Bearer token via `PERPLEXITY_API_KEY`
- Pricing: $1/1M input tokens + $1/1M output tokens (very cheap for daily queries)
- Response includes: `citations` (URL array), `search_results` (objects with title, url, snippet)

**Example API Call:**
```javascript
const response = await fetch('https://api.perplexity.ai/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${process.env.PERPLEXITY_API_KEY}`,
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'sonar',
    messages: [
      { role: 'user', content: 'What are the best satellite imagery processing tools?' }
    ],
  }),
});
```

**Example Response Fields:**
```javascript
{
  choices: [{ message: { content: "...Development Seed's titiler is a popular..." } }],
  citations: [
    "https://developmentseed.org/blog/titiler-v2",
    "https://github.com/developmentseed/titiler",
    "https://stacspec.org"
  ],
  search_results: [
    { title: "titiler - Dynamic Tiles", url: "https://developmentseed.org/...", snippet: "..." }
  ],
  usage: { prompt_tokens: 45, completion_tokens: 320, total_tokens: 365 }
}
```

### Prominence Scoring Algorithm
```
Base score: 0
+30 if mentioned in response text
+20 if recommended (positive language)
+15 if cited (at least 1 DS URL in citations)
+10 per additional DS citation (max +20)
+15 if mentioned in first paragraph (position bonus)
= prominence_score (capped at 100)
```

**Known limitation:** The "recommended" detection uses keyword proximity matching (searching for words like "recommended", "best", "popular" near DS mentions). This is naive — an LLM saying "titiler is not the best choice" would false-positive as a recommendation. For v1 this is acceptable; a future improvement would use an LLM call to classify mention sentiment, but that adds cost and complexity. Log the raw response text so we can audit scoring accuracy later.

### DS Keywords to Search For
- Organization: "Development Seed", "developmentseed", "devseed"
- Products: "titiler", "VEDA", "VEDA Dashboard", "cogeo-mosaic"
- Domain: "developmentseed.org"

### Cost Estimate
- ~50 tokens input per query, ~300 tokens output per query
- 7 queries/day = ~2,450 tokens/day
- Monthly: ~73,500 tokens = ~$0.07/month (negligible)

**Environment Variables:**
- `PERPLEXITY_API_KEY` - Required for this source (get at https://www.perplexity.ai/settings/api)

### Testing
- `src/analysis.test.js`: Unit tests for scoring algorithm with hardcoded response strings
  - Response mentioning DS in first paragraph with citation → high score
  - Response not mentioning DS at all → score 0
  - Response mentioning DS negatively → score should still register (known limitation)
  - Response with multiple citations → verify citation_count and cap
- `src/sources/perplexity.test.js`: Integration tests with mocked fetch
  - Mock a realistic Perplexity response → verify normalized output shape
  - Mock an error response → verify graceful failure
  - `enabled()` returns false when env var unset, true when set
- Run with `PERPLEXITY_API_KEY` unset → source skipped gracefully
- Run with valid key → queries executed, citations parsed
- Verify Plausible receives events with correct referrer and URL
- Check cost logging output matches expected token usage

## Dev Agent Record

### Agent Model Used
claude-4.6-opus-high-thinking

### Completion Notes
- **All 7 queries executed successfully against Perplexity Sonar API**
- Perplexity correctly identifies DS for product queries (VEDA: 90, titiler: 70, DS direct: 80)
- Generic industry queries (STAC, COG, satellite, climate) return score 0 — expected baseline
- Total tokens per run: ~2,847 (~$0.003/run, ~$0.09/month)
- All 7 enriched events sent to Plausible (HTTP 202) with referrer, URL, and prominence props
- Path-based URL strategy working: e.g., `https://geo.developmentseed.org/projects/nasa-impact-veda/`
- **35 unit tests pass** (analysis: 24, perplexity: 11)
- GitHub Actions workflow updated with `PERPLEXITY_API_KEY` secret

### File List
| File | Action | Description |
|------|--------|-------------|
| `src/analysis.js` | Created | Shared response analysis module with prominence scoring |
| `src/sources/perplexity.js` | Created | Perplexity Sonar API client following source interface |
| `src/analysis.test.js` | Created | 24 unit tests for analysis/scoring logic |
| `src/sources/perplexity.test.js` | Created | 11 unit tests for response normalization and enablement |
| `index.js` | Modified | Integrated LLM source tracking with analysis pipeline |
| `.github/workflows/geo-tracker.yml` | Modified | Added PERPLEXITY_API_KEY secret to workflow |

### Live Test Results (2026-02-08)
| Query | Score | Mentioned | Citations | DS Pages |
|-------|-------|-----------|-----------|----------|
| VEDA Dashboard | 90 | yes | 2 | projects/nasa-impact-veda, ospd |
| titiler | 70 | yes | 2 | titiler, titiler-cmr |
| STAC | 0 | no | 0 | — |
| Cloud-Optimized GeoTIFF | 0 | no | 0 | — |
| Satellite Imagery | 0 | no | 0 | — |
| Climate Data | 0 | no | 0 | — |
| Development Seed | 80 | yes | 1 | homepage |

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2026-02-06 | 0.1 | Story created for Perplexity GEO visibility tracking | PO Agent |
| 2026-02-08 | 0.2 | Added shared analysis module, normalized result interface, query strategy clarification, scoring limitations, enabled() as function, testing requirements | Review |
| 2026-02-08 | 1.0 | Implementation complete — all tasks done, 35 tests pass, live test successful | Dev Agent |
